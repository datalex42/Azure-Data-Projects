{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rag.jpg\" style=\"display:block; margin:auto\" width=350>\n", 
    "\n",
    "<div align=\"center\"> <h1 align=\"center\">Building a RAG-Based AI Chatbot for Document Search and Retrieval</h1> </div>\n",
    "\n",
    "This notebook outlines a comprehensive end-to-end workflow for building an AI-powered document search and retrieval system using Azure Document Intelligence, Cognitive Search, and OpenAI. The process involves several key steps:\n",
    "\n",
    "1. **Extracting Data**: Retrieve relevant document data from Azure Blob Storage, including labels and content, based on the automatically pre-labeled data with Azure Document Intelligence.\n",
    "2. **Index Creation**: Define and configure a custom search index in Azure Cognitive Search to store and organize the document data.\n",
    "3. **Generating Embeddings**: Use OpenAI's models to create vector embeddings for document content, enabling semantic search.\n",
    "4. **Uploading Data**: Upload the documents, including their embeddings, to Azure Cognitive Search for efficient retrieval.\n",
    "5. **Integrating AI-based Search**: Implement a search function that leverages both traditional and vector-based search, with responses generated by an AI-powered chatbot based on the retrieved documents using a Retrieval-Augmented Generation (RAG) model.\n",
    "\n",
    "This workflow combines cloud-based data storage, advanced search capabilities, and natural language processing (NLP) to facilitate interactive, context-driven conversations with the AI model. It allows users to ask questions and receive insightful answers based on document content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. Prerequisites & Install Relevant Packages\n",
    "\n",
    "- An Azure subscription, with access to Azure OpenAI, Azure Cognitive Search, Azure Document Intelligence.\n",
    "\n",
    "- A deployment of the text-embedding-3-large/text-embedding-ada-002 embedding model.\n",
    "\n",
    "- I used Python 3.12.5, Visual Studio Code with the Azure extension and test this example inside the Jupyter extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Required Libraries for Azure Integration and OpenAI\n",
    "\n",
    "We import essential libraries needed for interacting with Azure services, managing secrets, and working with OpenAI. These libraries are key to enabling the document storage, search, and retrieval capabilities, as well as integrating OpenAI for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid  \n",
    "import os  \n",
    "from dotenv import load_dotenv\n",
    "import json    \n",
    "from azure.storage.blob import BlobServiceClient  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from openai import AzureOpenAI\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents.indexes.models import (HnswParameters, \n",
    "                                                HnswAlgorithmConfiguration, \n",
    "                                                VectorSearchProfile, \n",
    "                                                SearchableField, \n",
    "                                                SearchField, \n",
    "                                                SearchFieldDataType, \n",
    "                                                SearchIndex, \n",
    "                                                SimpleField,\n",
    "                                                VectorSearch, \n",
    "                                                AzureOpenAIVectorizer,\n",
    "                                                AzureOpenAIParameters  \n",
    "                                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Loading Environment Variables for Secure Configuration\n",
    "\n",
    "First, we load environment variables from a `.env` file using the `load_dotenv` function, allowing secure storage of sensitive information such as API keys, endpoint names, and deployment configurations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True) # take environment variables from .env\n",
    "\n",
    "# Import variables from your .env file\n",
    "key_vault_name = os.getenv('key_vault_name')\n",
    "openai_secret_key_name = os.getenv('openai_secret_key_name')\n",
    "openai_endpoint_name = os.getenv('openai_endpoint_name')\n",
    "deployment_name = os.getenv('deployment_name') # gpt4o, gpt4, gpt3.5-turbo etc.\n",
    "search_endpoint_name = os.getenv('search_endpoint_name')\n",
    "search_endpoint_key = os.getenv('search_endpoint_key')\n",
    "use_embedding_deployment = os.getenv('use_embedding_deployment') # text-embedding-3-large, text-embedding-ada-002, text-embedding-3-small\n",
    "embedding_dimensions = int(os.getenv('embedding_dimensions'))\n",
    "azure_openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-06-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Setting Up Azure OpenAI Client with Authentication\n",
    "\n",
    "This code snippet initializes the Azure OpenAI client by setting up authentication using Azure credentials, API keys, and token providers, enabling access to OpenAI models and deployment configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(openai_credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_deployment=use_embedding_deployment,\n",
    "    api_version=azure_openai_api_version,\n",
    "    azure_endpoint=openai_endpoint_name,\n",
    "    api_key=openai_secret_key_name,\n",
    "    azure_ad_token_provider=token_provider if not openai_secret_key_name else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Creating a Custom Azure Search Index with Vector Search Configuration\n",
    "\n",
    "This code defines a function to create a custom Azure Cognitive Search index, with optional custom fields and vector search capabilities. It configures the index to include embedding vectors for advanced search features and ensures the correct setup of fields before indexing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_index(index_name, embedding_dimensions=1536, custom_fields=None, embedding_deployment=None):\n",
    "    \n",
    "    # Check if embedding deployment variable is provided\n",
    "    if embedding_deployment is None:\n",
    "        raise ValueError('Embedding deployment must be provided')\n",
    "\n",
    "    # Create the Search Index Client\n",
    "    index_client = SearchIndexClient(endpoint=search_endpoint_name, credential=AzureKeyCredential(search_endpoint_key))\n",
    "\n",
    "    # If no custom fields are provided, use default fields\n",
    "    if custom_fields is None:\n",
    "        # Default fields\n",
    "        default_fields = [\n",
    "            SimpleField(name='id', type=SearchFieldDataType.String, key=True, searchable=False, retrievable=False),\n",
    "            SearchableField(name='combined_content', type=SearchFieldDataType.String, retrievable=True, searchable=True, filterable=True),\n",
    "            SearchField(\n",
    "                name='combined_content_vector', \n",
    "                type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "                searchable=True, \n",
    "                retrievable=True,\n",
    "                vector_search_dimensions=embedding_dimensions, \n",
    "                vector_search_profile_name='vector_profile'\n",
    "            )\n",
    "        ]\n",
    "        fields_to_use = default_fields  # Use default fields if no custom fields provided\n",
    "    \n",
    "    else:\n",
    "        # Process the custom fields\n",
    "        searchable_fields = [\n",
    "            SearchField(\n",
    "                name=field['name'],\n",
    "                type=field['type'],\n",
    "                searchable=field.get('searchable', True),\n",
    "                filterable=field.get('filterable', True),\n",
    "                sortable=field.get('sortable', True),\n",
    "                retrievable=field.get('retrievable', True)\n",
    "            )\n",
    "            for field in custom_fields if field['name'] != 'id' and not field['name'].endswith('_vector')\n",
    "        ]\n",
    "\n",
    "        # Create the SimpleField for primary key (id)\n",
    "        simple_field = [\n",
    "            SimpleField(\n",
    "                name=field['name'],\n",
    "                type=field['type'],\n",
    "                key=True,\n",
    "                searchable=field.get('searchable', False),\n",
    "                retrievable=field.get('retrievable', False)\n",
    "            )\n",
    "            for field in custom_fields if field['name'] == 'id'\n",
    "        ]\n",
    "\n",
    "        # Create the Vector Field\n",
    "        vector_field = [\n",
    "            SearchField(\n",
    "                name=field['name'],\n",
    "                type=field['type'],\n",
    "                searchable=field.get('searchable', False),\n",
    "                retrievable=field.get('retrievable', True),\n",
    "                vector_search_dimensions=embedding_dimensions,\n",
    "                vector_search_profile_name='vector_profile'\n",
    "            )\n",
    "            for field in custom_fields if field['name'].endswith('_vector')\n",
    "        ]\n",
    "\n",
    "        # Combine fields into one list for custom fields\n",
    "        fields_to_use = simple_field + searchable_fields + vector_field\n",
    "\n",
    "    # Ensure there are fields to pass to the SearchIndex\n",
    "    if not fields_to_use:\n",
    "        raise ValueError('No fields defined for the index.')\n",
    "\n",
    "    # Define Vector Search configuration\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[HnswAlgorithmConfiguration(name='algorithm', parameters=HnswParameters(metric='cosine'))],\n",
    "        profiles=[VectorSearchProfile(name='vector_profile', algorithm_configuration_name='algorithm', vectorizer='Vectorizer')],  \n",
    "        vectorizers=[AzureOpenAIVectorizer(  \n",
    "            name='Vectorizer',  \n",
    "            azure_open_ai_parameters=AzureOpenAIParameters(  \n",
    "                resource_uri=openai_endpoint_name,   \n",
    "                deployment_id=embedding_deployment,\n",
    "                model_name=embedding_deployment, \n",
    "                api_key=openai_secret_key_name)\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the SearchIndex object, using fields_to_use\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields_to_use,  # Pass either default or custom fields\n",
    "        vector_search=vector_search\n",
    "    )\n",
    "\n",
    "    # Try to create or update the index\n",
    "    try:\n",
    "        result = index_client.create_or_update_index(index)\n",
    "        print(f\"Index '{result.name}' created successfully.\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error creating or updating index: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Defining Custom Fields for Azure Cognitive Search \n",
    "\n",
    "Based on the `create_custom_index` function, we now create custom fields for an Azure Cognitive Search index, specifying the field types, searchability, retrievability, and vector search settings. It includes essential fields like `id`, `title`, and `author`, along with a `combined_content_vector` for advanced vector-based search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example custom_field usage\n",
    "set_custom_fields = [\n",
    "    {'name': 'id', 'type': SearchFieldDataType.String, 'key': True, 'retrievable': False, 'analyzer_name': 'keyword'},\n",
    "    {'name': 'title', 'type': SearchFieldDataType.String, 'searchable': True, 'filterable': True},\n",
    "    {'name': 'combined_content', 'type': SearchFieldDataType.String, 'searchable': True, 'filterable': True, 'retrievable' : True},\n",
    "    {'name': 'author', 'type': SearchFieldDataType.String, 'searchable': True, 'filterable': True},\n",
    "    {'name': 'published_date', 'type': SearchFieldDataType.String, 'searchable': True, 'sortable': True},\n",
    "    {'name': 'combined_content_vector', 'type': SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "     'searchable': True, 'retrievable': True,\n",
    "     'vector_search_dimensions': embedding_dimensions, 'vector_search_profile_name': 'vector_profile'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4 Creating the Custom Index with Defined Fields and Embedding Deployment\n",
    "\n",
    "This function call creates a custom Azure Cognitive Search index named `'test_index'` with specified custom fields and embedding dimensions. It also integrates the selected embedding deployment to enable vector-based search functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_custom_index('test_index', embedding_dimensions=1536, custom_fields=set_custom_fields, embedding_deployment=use_embedding_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Downloading and Extracting Labels from Azure Blob Storage\n",
    "\n",
    "These functions download data from Azure Blob Storage and extract labels from `.json` files. The `download_blob_to_string` function retrieves the content of a specific blob, while `extract_labels_from_blob` lists all blobs in a container, filters based on specified criteria, and extracts labels from valid `.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob_to_string(container_client, blob_name):  \n",
    "    try:  \n",
    "        blob_client = container_client.get_blob_client(blob_name)  \n",
    "        blob_data = blob_client.download_blob().readall()  \n",
    "        return blob_data.decode('utf-8')  \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading blob {blob_name}: {e}\")  \n",
    "        return None  \n",
    "  \n",
    "def extract_labels_from_blob(blob_url, sas_token, container_name, filter_value):  \n",
    "    try:  \n",
    "        blob_service_client = BlobServiceClient(account_url=blob_url, credential=sas_token)  \n",
    "        container_client = blob_service_client.get_container_client(container_name)  \n",
    "          \n",
    "        labels = []  \n",
    "          \n",
    "        # List all blobs in the container  \n",
    "        blob_list = container_client.list_blobs()  \n",
    "        for blob in blob_list:  \n",
    "            if blob.name.endswith('.json') and all(value not in blob.name for value in filter_value):\n",
    "                blob_content = download_blob_to_string(container_client, blob.name)  \n",
    "                if blob_content:  \n",
    "                    labels.append(json.loads(blob_content))\n",
    "\n",
    "        return labels  \n",
    "  \n",
    "    except Exception as e:\n",
    "        print(e)  \n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 Extracting Entities and Compiling Documents with Labels\n",
    "\n",
    "Next, we define two functions: `extract_entitie`s and `compile_documents`. The `extract_entities` function extracts and arranges text from labeled entities into a unified format. The `compile_documents` function organizes documents with their respective labels into separate dictionaries, ensuring proper structure based on provided title keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange all text with labels in one line, remove fragmanted status\n",
    "def extract_entities(data):  \n",
    "    entities = []  \n",
    "    for document in data:  \n",
    "        for item in document.get('labels', []):  \n",
    "            label = item['label']  \n",
    "            entity_texts = [v['text'] for v in item['value']]  \n",
    "            entity = ' '.join(entity_texts)  \n",
    "            entities.append({label: entity})  \n",
    "    return entities  \n",
    "\n",
    " # Compile documents with their labels in separate dictionaries  \n",
    "def compile_documents(entities, title_keys):  \n",
    "    if not title_keys:\n",
    "        raise ValueError(\"title_keys must be provided and cannot be None or empty.\")\n",
    "\n",
    "    compiled_docs = []  \n",
    "    current_doc = {}  \n",
    "\n",
    "    for item in entities:  \n",
    "        # Check if the item contains any of the title keys\n",
    "        if any(key in item for key in title_keys):  \n",
    "            if current_doc:  # Save the previous document  \n",
    "                compiled_docs.append(current_doc)  \n",
    "            current_doc = item  # Start a new document  \n",
    "        else:  \n",
    "            current_doc.update(item)  # Add other labels to the current document\n",
    "    \n",
    "    if current_doc:  # Add the last document  \n",
    "        compiled_docs.append(current_doc)  \n",
    "    \n",
    "    return compiled_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Extracting and Compiling Documents from Blob Storage with Error Handling\n",
    "\n",
    "In thhis code block we use the functions created before to extract the data from Azure Blob Storage using a SAS token, processes the entities from the data, and organizes them into structured documents. We handle potential errors and print relevant messages when no data or entities are found. The `compile_documents` function is used to group entities into documents based on provided title keys, and the results are printed out for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  \n",
    "    # Configuration parameters  \n",
    "    blob_url =  os.getenv('url-of-blob') \n",
    "    sas_token = os.getenv('your-sas-token-of-blob-container')\n",
    "    container_name = os.getenv('your-container-name') \n",
    "          \n",
    "    filter_value = ['config', 'fields', 'ocr'] # Adjust filter_value to just include files with specific names \n",
    "          \n",
    "    # Extract labels from blob storage  \n",
    "    data = extract_labels_from_blob(blob_url, sas_token, container_name, filter_value)  \n",
    "    if not data:  \n",
    "        print('No data extracted from blob storage.')  \n",
    "        exit()  \n",
    "          \n",
    "    # Extract entities  \n",
    "    entities = extract_entities(data)  \n",
    "    if not entities:  \n",
    "        print('No entities extracted.')  \n",
    "        exit()  \n",
    "    \n",
    "# Example entity data! - use first key of true data and pass to title_keys parameter\n",
    "\n",
    "#     entities = [\n",
    "#     {'document_title': 'Document 1', 'author': 'Author 1'},\n",
    "#     {'section': 'Introduction', 'content': 'This is the introduction.'},\n",
    "#     {'document_title': 'Document 2', 'author': 'Author 2'},\n",
    "#     {'section': 'Summary', 'content': 'This is the summary.'}\n",
    "# ]\n",
    "\n",
    "    # Call the function with mandatory title keys\n",
    "    compiled_documents = compile_documents(entities, title_keys=['your_first_document_key'])\n",
    "    if not compiled_documents:  \n",
    "        print('No compiled documents generated.')  \n",
    "        exit()  \n",
    "          \n",
    "    # Print compiled documents  \n",
    "    for i, doc in enumerate(compiled_documents):  \n",
    "        print(f'Document {i}: {doc}')\n",
    "            \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Ensuring All Documents Have Expected Keys\n",
    "\n",
    "Here we iterate through the compiled documents and ensure that each document contains all keys listed in `expected_key_list`. If any key is missing, it adds the key with a default value of `'Null'`. This ensures that all documents are consistently structured with the expected fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_keys(compiled_documents, expected_key_list):\n",
    "    for compiled_doc in compiled_documents:  \n",
    "    # Ensure each document has all expected keys with empty values if missing  \n",
    "        for expected_key in expected_key_list:  \n",
    "            if expected_key not in compiled_doc:  \n",
    "                compiled_doc[expected_key] = 'Null'\n",
    "\n",
    "    return compiled_documents\n",
    "\n",
    "expected_key_list = [\n",
    "    'id',\n",
    "    'title', \n",
    "    'author',  \n",
    "    'published_date',\n",
    "]\n",
    "\n",
    "compiled_documents = expected_keys(compiled_documents, expected_key_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Generating Document Embeddings and Preparing Data for Upload\n",
    "\n",
    "In this code block we generate embeddings for each compiled document by combining its content and creating a vector representation using Azure OpenAI. It processes the documents, appends embeddings, and prepares them for upload, ensuring each document has a unique ID and includes both the original data and the computed vector. The documents are stored in the `final_documents` list, ready for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(  \n",
    "    azure_endpoint=openai_endpoint_name,  \n",
    "    api_key=openai_secret_key_name,  \n",
    "    api_version='2024-05-01-preview'\n",
    ") \n",
    "\n",
    "# Prepare final documents for upload  \n",
    "final_documents = []\n",
    "\n",
    "for compiled_doc in compiled_documents:  # Make sure to use the correct variable here (compiled_documents)\n",
    "              \n",
    "    # Create a combined_content field for the document dynamically based on available keys\n",
    "    combined_content = ' '.join([compiled_doc[key] for key in expected_key_list if key in compiled_doc])  \n",
    "              \n",
    "    # Generate embeddings for the combined_content field  \n",
    "    response = client.embeddings.create(input=combined_content, model=use_embedding_deployment)  \n",
    "    combined_content_vector = response.data[0].embedding  \n",
    "              \n",
    "    # Create a new dictionary to hold the entire dataset, including combined content and vector  \n",
    "    final_data = {\n",
    "        'id': str(uuid.uuid4()),  # Generate a unique ID for the entire document\n",
    "        'combined_content': combined_content,  \n",
    "        'combined_content_vector': combined_content_vector\n",
    "    }\n",
    "    \n",
    "    # Dynamically add all keys from compiled_doc\n",
    "    for key, value in compiled_doc.items():\n",
    "        final_data[key] = value  # Add each key-value pair from compiled_doc to final_data\n",
    "              \n",
    "    final_documents.append(final_data)  # Add the processed document to final_documents\n",
    "          \n",
    "# Print the final documents structure  \n",
    "for i, doc in enumerate(final_documents):  \n",
    "    print(f'Final Document {i}: {doc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Saving and Loading Documents with Embeddings in JSON Format\n",
    "\n",
    "This code block saves the processed documents, including their embeddings, to a JSON file (here: `vector_data.json`) in a specified output directory. It ensures the directory exists before saving. After saving, it reads the same JSON file back into memory to access the stored documents for further processing or querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output combined data to json file  \n",
    "output_path = os.path.join('..', 'output', 'vector_data.json')  \n",
    "output_directory = os.path.dirname(output_path)  \n",
    "\n",
    "if not os.path.exists(output_directory):  \n",
    "    os.makedirs(output_directory)\n",
    "      \n",
    "with open(output_path, 'w') as f:  \n",
    "    json.dump(final_documents, f)  \n",
    "          \n",
    "# Read the combined data from docVectors.json file  \n",
    "with open(output_path, 'r') as file:  \n",
    "    documents = json.load(file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2. Uploading Documents to Azure Cognitive Search\n",
    "\n",
    "After reading the JSON, we upload documents to an Azure Cognitive Search index. It ensures that documents are properly formatted as a list (in case only a single document is passed as a dictionary) and then uploads them using the `SearchClient`. After the upload, it prints the number of documents successfully uploaded to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_client = SearchClient(endpoint=search_endpoint_name, index_name='test_index', credential=AzureKeyCredential(search_endpoint_key))  \n",
    "          \n",
    "# Ensure the documents are wrapped in a list  \n",
    "if isinstance(documents, dict):  \n",
    "    documents = [documents]  # Wrap the single dictionary in a list  \n",
    "          \n",
    "# Upload documents to Azure Cognitive Search  \n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f'Uploaded {len(documents)} documents')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Performing Vector-Based Search on Azure Cognitive Search\n",
    "\n",
    "Now, we can connect to the index and perform the first vector-based search on the Azure Cognitive Search index. This code creates a query using the input text, searches the `combined_content_vector` field for the nearest neighbors, and retrieves results based on the query. The function processes the results and returns relevant fields such as `title`, `content`, `author`, and `publication date`, along with the `search score` for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SearchClient with the endpoint, index name, and credentials  \n",
    "search_client = SearchClient(endpoint=search_endpoint_name, index_name='test_index', credential=AzureKeyCredential(search_endpoint_key))  \n",
    "\n",
    "def perform_search(query_text):  \n",
    "    # Create a vector query with the entered text and the top k nearest neighbors  \n",
    "    vector_query = VectorizableTextQuery(  \n",
    "        text=query_text,  \n",
    "        fields='combined_content_vector',  \n",
    "        k_nearest_neighbors=3,  \n",
    "        exhaustive=True  \n",
    "    )  \n",
    "    \n",
    "    try:\n",
    "        # Perform the search  \n",
    "        results = search_client.search(  \n",
    "            search_text=query_text,  # The entered search text  \n",
    "            vector_queries=[vector_query],  # The vector-based query  \n",
    "            select=expected_keys,  # Fields to be returned in the result  \n",
    "            include_total_count=True  # Include the total count of the results  \n",
    "        )  \n",
    "    \n",
    "        result_list = []  \n",
    "        # Iterate over the search results, include all fields  \n",
    "        for result in results:  \n",
    "            result_dict = {  \n",
    "                'score': result['@search.score'],  # Include search score  \n",
    "                'title': result.get('title', '').strip(),  # Include 'title' field\n",
    "                'combined_content': result.get('combined_content', '').strip(),  \n",
    "                'author': result.get('author', '').strip(),  # Include 'author' field\n",
    "                'published_date': result.get('published_date', '')  # Include 'published_date' field\n",
    "            } \n",
    "            result_list.append(result_dict)  \n",
    "        \n",
    "        return result_list\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Failed response: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Creating a Prompt for OpenAI Chat Completion\n",
    "\n",
    "This function generates a prompt for OpenAI Chat Completion by formatting the search results and including the user's query. It organizes the search results into a readable structure and appends the user’s query, asking how to assist further based on the retrieved information. This prompt is then passed to OpenAI to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the prompt for OpenAI Chat Completion  \n",
    "def create_prompt(results, user_input):  \n",
    "    prompt = 'Here are some search results based on your query:\\n\\n'  \n",
    "    for i, result in enumerate(results):  \n",
    "        prompt += f'Result {i+1}:\\n'  \n",
    "        for key, value in result.items():  \n",
    "            prompt += f'{key}: {value}\\n'  \n",
    "        prompt += '\\n'  \n",
    "    prompt += f'User query: {user_input}\\nHow can I assist you further based on these results?'  \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Getting a Response from OpenAI Chat API\n",
    "\n",
    "The `get_openai_response` function sends the formatted prompt to the OpenAI model (e.g., gpt4o) to generate a response. It configures the request with a system message and the user's query, specifies parameters like `max_tokens` for output length, and adjusts creativity via the `temperature` setting. The function returns the generated response from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get response from OpenAI  \n",
    "def get_openai_response(prompt):  \n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(  \n",
    "            model='gpt4o',    # Ensure this matches the deployment name in Azure  \n",
    "            messages=[  \n",
    "                {'role': 'system', 'content': 'You are an assistant for any problem'}, # Adjust this value for personal preferences and based on documents \n",
    "                {'role': 'user', 'content': prompt}  \n",
    "            ],  \n",
    "            max_tokens=400,  # Adjust output length\n",
    "            n=1,  # Generate one output message\n",
    "            stop=None,  # Up to 4 sequences where the API will stop generating further tokens.\n",
    "            temperature=0.3,  # Set creativity of AI\n",
    "        )  \n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Failed response: {e}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Managing a Conversation with OpenAI Chat API\n",
    "\n",
    "Finally we can initiate a conversation with the OpenAI model by sending an initial query and generating a response. The code is created as a loop where user input is continuously processed. For each input, a search is performed, a new prompt is created, and a response is generated. The conversation context is stored, and the loop continues until the user decides to exit. The conversation history is updated after every interaction, ensuring continuity in the discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial interaction  \n",
    "initial_query = 'I will asks you questions'  # Replace with other initial query if needed  \n",
    "results = perform_search(initial_query)  \n",
    "prompt = create_prompt(results, initial_query)  \n",
    "chat_response = get_openai_response(prompt)  \n",
    "print('Chat Completion Response:', chat_response, flush=True) # Ensure immediate output  \n",
    "\n",
    "conversation_history = []\n",
    "# Continue the conversation  \n",
    "while True:  \n",
    "    user_input = input('You: ')  \n",
    "    if user_input.lower() in ['exit', 'quit']:  \n",
    "        print('Ending the conversation.', flush=True)  \n",
    "        break  \n",
    "\n",
    "    # Store conversation context   \n",
    "    conversation_history.append({'role': 'user', 'content': user_input}) \n",
    "    # Perform a new search based on user input  \n",
    "    results = perform_search(user_input)  \n",
    "      \n",
    "    # Create a new prompt based on user input and search results  \n",
    "    new_prompt = create_prompt(results, user_input)  \n",
    "    chat_response = get_openai_response(new_prompt)\n",
    "    conversation_history.append({'role': 'assistant', 'content': chat_response}) \n",
    "\n",
    "    print('AI: ', chat_response, flush=True)  # Ensure immediate output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
